{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 1: Insert a project token and run the inserted code cell\n<br/>A project token will allow you to access all the resources defined within this project.  By default, the token is inserted into the very first cell in a notebook.\n<br/><img style=\"float: left;\" src=\"https://github.com/yfphoon/dsx_local/blob/master/images/project_token.png?raw=true\" alt=\"Project Token\" />"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this notebook you will learn how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML). \n\nThis notebook walks you through these steps:\n- Build a model with SparkML API\n- Save the model in the ML repository\n- Create a Deployment in ML (via UI)\n- Test the model (via UI)\n- Test the model (via REST API)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Use Case"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "The analytics use case implemented in this notebook is telco churn. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics. \n![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n\nThe analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n\nOnce the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"churn\" or \"no churn\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. \n\nFinally, if the model's accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn.  \n\nWe also developed a sample Python Flask application to illustrate deployment: http://predictcustomerchurn.mybluemix.net/. This application implements the REST client call to the model."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Working with Notebooks"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. To run the notebook, it must be in the Edit mode. If you don't see the menu in the notebook, then it's not in the edit mode. Click on the pencil icon.\n2. The notebook has 2 types of cells - markdown (text) and code. \n3. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n4. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n5. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 2: Load data "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "If the first step ran successfully (you saw the output), then continue reviewing the notebook and running each code cell step by step. Note that not every cell has a visual output. The cell is still running if you see a * in the brackets next to the cell. \n\nIf the first step didn't finish successfully, check with the instructor. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 3: Merge Files"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "data=customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 4: Rename some columns\nThis step is to remove spaces from columns names, it's an example of data preparation that you may have to do before creating a model. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.toPandas().head()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 5: Data understanding"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "Data preparation and data understanding are the most time-consuming tasks in the data mining process. The data scientist needs to review and evaluate the quality of data before modeling.\n\nVisualization is one of the ways to reivew data.\n\nThe Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. \nMore information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here: http://brunel.mybluemix.net/gallery_app/renderer"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import brunel\ndf = data.toPandas()\n%brunel data('df') \\\n        bar x(CHURN, LocalBilltype) y(EstIncome) mean(EstIncome) color(LocalBilltype)  tooltip(EstIncome) | \\\n        point x(LongDistance) y(Usage) color(Paymethod) tooltip(LongDistance, Usage)\\\n        :: width=1100, height=400 ", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an **interactive UI** in which you can explore data.\n\nMore information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart", 
                        "rowCount": "500", 
                        "keyFields": "Paymethod", 
                        "rendererId": "matplotlib", 
                        "chartsize": "50", 
                        "valueFields": "Usage", 
                        "aggregation": "SUM"
                    }
                }
            }, 
            "source": "from pixiedust.display import *\ndisplay(data)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 6: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models.\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Prepare string variables so that they can be used by the decision tree algorithm\n# StringIndexer encodes a string column of labels to a column of label indices\nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n\n#encode the Label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)\n\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \"LongDistanceBilltypeEncoded\",\\\n                                       \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                       \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, assembler, rf, labelConverter])", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "# Split data into train and test datasets\ntrain, test = data.randomSplit([0.8,0.2], seed=6)\ntrain.cache()\ntest.cache()", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(train)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "### Step 7: Score the test data set"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "results = model.transform(test)\nresults=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\nresults.toPandas().head(6)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 8: Model Evaluation "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "We have finished building and testing a predictive model. The next step is to deploy it for real time scoring. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 9: Persist model"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using python client libraries.\n\nFirst, you must import client libraries.\n\n**Note**: Apache\u00ae Spark 2.0 or higher is required."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "Authenticate to Watson Machine Learning service on Bluemix.\n\n**Action**: Put authentication information from your instance of Watson Machine Learning service here.</div>"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "wml_credentials={\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"access_key\": \"X76Yu8/fV526oP70L7tQ38+kReWmaFiXJmruwas0z0Kxw13lGFyPu/tsKh3P0I5IHxGxQ3pIogjgEOjN0TGDTcL0h32gVzPkwMbmHXNpi+FQYUqQmv73SQJrb1WXWeZv\",\n  \"username\": \"65a3184a-73f1-4619-830b-67fdade086f8\",\n  \"password\": \"70f9d64b-a157-4f9b-a1f8-be77092748a5\",\n  \"instance_id\": \"e4e074bd-fed8-45c3-af30-abaede0466f8\"\n}", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: `wml_service_path`, user and `wml_password` can be found on **Service Credentials** tab of service instance created in Bluemix. If you cannot see **instance_id** field in **Serice Credentials** generate new credentials by pressing **New credential (+)** button. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "ml_repository_client = MLRepositoryClient(wml_credentials['url'])\nml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Create model artifact (abstraction layer)."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Telco Churn Prediction\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 10: Save pipeline and model"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "saved_model = ml_repository_client.models.save(model_artifact)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Get saved model metadata from Watson Machine Learning."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: Use *meta.available_props()* to get the list of available props."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "saved_model.meta.available_props()", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\"))\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Tip**: **modelVersionHref** is our model unique indentifier in the Watson Machine Learning repository."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Step 11: Load model"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this subsection you will learn how to load back saved model from specified instance of Watson Machine Learning."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You can print for example model name to make sure that model has been loaded correctly."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "print str(loadedModelArtifact.name)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "As you can see the name is correct. You have already learned how save and load the model from Watson Machine Learning repository."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "m = loadedModelArtifact.model_instance()", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "m.stages", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "type(m.stages[1])", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"scoring\"></a>\n### Step 12: Deploy and score in a Cloud"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "In this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \nFor more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "To work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password']))\nurl = '{}/v3/identity/token'.format(wml_credentials['url'])\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Create online scoring endpoint"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now you can create an online scoring endpoint. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Get published_models url from instance details"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "endpoint_instance = wml_credentials['url'] + \"/v3/wml_instances/\" + wml_credentials['instance_id']\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get_instance = requests.get(endpoint_instance, headers=header)\n\nprint response_get_instance\nprint response_get_instance.text", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url')\n\nprint endpoint_published_models", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Execute the following sample code that uses the published_models endpoint to get deployments url."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Get the list of published models"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\nresponse_get = requests.get(endpoint_published_models, headers=header)\n\nprint response_get\nprint response_get.text", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Get published model deployment url"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid]\n\nprint endpoint_deployments", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Create online deployment for published model"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "payload_online = {\"name\": \"Product Line Prediction\", \"description\": \"My Cool Deployment\", \"type\": \"online\"}\nresponse_online = requests.post(endpoint_deployments, json=payload_online, headers=header)\n\nprint response_online\nprint response_online.text", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "scoring_url = json.loads(response_online.text).get('entity').get('scoring_url')\nprint scoring_url", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now, you can send (POST) new scoring records (new data) for which you would like to get predictions. To do that, execute the following sample code: "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "payload_scoring = {\"fields\": [\"Gender\",\"Status\",\"Children\",\"EstIncome\",\"CarOwner\",\"Age\",\"LongDistance\",\"International\",\"Local\",\"Dropped\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\",\"Usage\",\"RatePlan\"],\\\n                   \"values\": [[\"F\",\"S\",1.000000,38000.000000,\"N\",24.393333,23.560000,0.000000,206.080000,0.000000,\"CC\",\"Budget\",\"Intnl_discount\",229.640000,3.000000]]}\nresponse_scoring = requests.post(scoring_url, json=payload_scoring, headers=header)\n\nprint response_scoring.text", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Summary"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": " You successfully completed this notebook! You learned how to use Apache Spark machine learning as well as Watson Machine Learning for model creation and deployment. Check out our _[Online Documentation](https://console.ng.bluemix.net/docs/services/PredictiveModeling/pm_service_api_spark.html)_ for more samples, tutorials, documentation, how-tos, and blog posts. "
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 1
}